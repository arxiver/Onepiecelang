{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"  \n",
    "### Title: M.I. assignment problem no. 1  \n",
    "### Author: Mohamed Mokhtar Abdelrazek  \n",
    "### Section: 2  \n",
    "### B.N.: 17  \n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unigrams(path):\n",
    "    \"\"\"\n",
    "    Reading the words dataset and store it into a dictionary\n",
    "    and setting weight for each word using log of (words_count - word_order)\n",
    "    higher order, better weight.\n",
    "    Args:\n",
    "        path: string of the path of the file.\n",
    "    Returns:\n",
    "        words: dict of the words and each with a given weight.\n",
    "        max_len: maximum length of words in the dict.\n",
    "    \"\"\"\n",
    "    words = {}\n",
    "    max_len = 0\n",
    "    file = open(path).read().split()\n",
    "    words_count = len(file)\n",
    "    words = {}\n",
    "    for (i,word) in enumerate(file):\n",
    "        words[word] = np.log(i+1)\n",
    "        if (len(word) > max_len):\n",
    "            max_len = len(word)\n",
    "    return words, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spaces(sentence, unigrams, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentence: string the sentence/url before processing (concatenated sentence)\n",
    "        unigrams: dict the words in the data-set with corresponding weight\n",
    "        window_size: maximum word length in the dict\n",
    "    Returns:\n",
    "        separated: list of strings words after separation \n",
    "    \"\"\"\n",
    "    # s_lower: string holds the lowercase of the sentence because the dict is built on lowercase words\n",
    "    sentence = clean_sentence(sentence)\n",
    "    sentence = sentence.replace(' ','')\n",
    "    s_lower = sentence.lower()\n",
    "    weights = [0]\n",
    "    splits = []\n",
    "    separated = []\n",
    "    # i represnts the end of the sentence which i will look before\n",
    "    # For every letter from [i=1 to len(s)+1] and try to split each word at each letter\n",
    "    for i in range(1, len(s_lower)+1):\n",
    "        w_min, w_count = minimize_sentence_cost(s_lower, unigrams, i, weights, window_size)\n",
    "        weights.append(w_min)\n",
    "        splits.append(w_count)\n",
    "\n",
    "    i = len(sentence)\n",
    "    while i > 0:\n",
    "        j = splits[i-1]\n",
    "        separated.append(sentence[i-j:i])\n",
    "        i -= j\n",
    "        \n",
    "    separated = list(reversed(separated))\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_sentence_cost(sentence, unigrams, i, weights, window_size):\n",
    "    \"\"\"\n",
    "    Minimize the sentence cost at the current index by looking backwards and trying to find best word \n",
    "    can minimize the cost of the sentence \n",
    "    e.g. sentence: He is good, and my ptr is standing at 's'\n",
    "    It will try to split all previous letters in window of size \n",
    "    Args:\n",
    "        sentence: string the sentence concatenated without spaces.\n",
    "        unigrams: dict each word as key in the data-set and its cost as value.\n",
    "        i: int the end of the ptr of current processign word.\n",
    "        weights: list the cost of the sentence at each index with respect to each split.\n",
    "        window_size: int maximum word length in the unigrams.\n",
    "    Returns:\n",
    "        cost_min: float the new cost of the sentence with respsect to 'i' letter's split happend.\n",
    "        cost_min_j: int the count of letters taken with backwards from 'i' to make this minmum cost.\n",
    "    \"\"\"\n",
    "    # i is the end of the ptr of current processing word\n",
    "    # We look before it letter by letter and split the subword incrementally,\n",
    "    # and trying to find subword which minimize cost of sentence, \n",
    "    # and cost of splitting this word in addition of to the cost of sentence\n",
    "    start = max(0, i-window_size)\n",
    "    prefix_costs = weights[start:i]\n",
    "    cost_min, letters_count = 1000, 0\n",
    "    \"\"\"\n",
    "    Minimize [cost(left_sub_word) + cost(right_sub_word)]\n",
    "    Minmum cost of sentence when took this split cost(left) + cost(right)\n",
    "    W_j: W_0j (sub-word from letter 0 to letter j), \n",
    "         Cost of the prefix word from current_letter-j-max_word_length to current_letter-j\n",
    "    W_ji = W_ji (sub-word from letter j to letter i-1)\n",
    "    Update new minimum cost\n",
    "    \"\"\"\n",
    "    for j in range(len(prefix_costs)):\n",
    "        # Weight of sub_word at previous of i [-ive] access e.g. j=0 \n",
    "        # e.g. preifixs[-1] means i take letter at i-1 as a letter only and rest of word's cost\n",
    "        w_j =  prefix_costs[-(j+1)] \n",
    "        # Weight of sub_word j->i\n",
    "        w_ji = unigrams.get(sentence[i-j-1:i]) \n",
    "        # If there is a matching word that preceeds i with j_letters_count \n",
    "        # And cost of (word_0j + word_ji) < cost_of_min_split_happend_before\n",
    "        # Update cost\n",
    "        if w_ji != None and cost_min > (w_j + w_ji):\n",
    "            # Update new cost\n",
    "            cost_min = w_j + w_ji\n",
    "            # length of prefix letters included with letter i to make minmium cost of sentence\n",
    "            letters_count = j+1\n",
    "    return cost_min, letters_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(s):\n",
    "    # remove anything except a..z and A..Z and space and dots.\n",
    "    return re.sub(\"[^a-zA-Z .]\",\"\",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X, Y, unigrams, max_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: list of sentences \n",
    "        Y: list of lists, for each sentence its corresponding actual output separated list.\n",
    "        unigrams: dictionary the words in the data-set with corresponding weight\n",
    "        window_size: maximum word length in the dictionary\n",
    "\n",
    "    Returns:\n",
    "        score: number of correctly separated words in exact place corresponding \n",
    "        to same place and value in the actual output list \n",
    "        divided by the total number of tested words, and multiplied by 100\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    correct = 0\n",
    "    correct_sentences = 0\n",
    "    for i in range(len(X)):\n",
    "        total_words += len(Y[i])\n",
    "        y_hat = find_spaces(X[i],unigrams, max_len)\n",
    "        ptr_hat = 0\n",
    "        ptr_tmp = 0\n",
    "        for word in Y[i]:\n",
    "            if ptr_hat >= len(y_hat):\n",
    "                break\n",
    "            elif(word == y_hat[ptr_hat]):\n",
    "                correct +=1\n",
    "                ptr_hat +=1\n",
    "            else:\n",
    "                ptr_tmp = ptr_hat\n",
    "                while (ptr_tmp < len(y_hat) and y_hat[ptr_tmp] != word):\n",
    "                    ptr_tmp +=1 \n",
    "                if (ptr_tmp != len(y_hat)):\n",
    "                    ptr_hat = ptr_tmp + 1\n",
    "                    correct +=1\n",
    "        if(ptr_hat == len(Y[i])):\n",
    "            correct_sentences +=1\n",
    "    print(\"No. of sentences:\", len(X))\n",
    "    print(\"No. of total words:\", total_words)\n",
    "    print(\"No. of correct words:\", correct)\n",
    "    print(\"No. of completely correct sentences:\", correct_sentences)\n",
    "    return float(\"{:.2f}\".format((correct/total_words) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_testset(path):\n",
    "    \"\"\"\n",
    "    Reading the words dataset and store it into a dictionary\n",
    "    and setting weight for each word using log of (words_count - word_order)\n",
    "    higher order, better weight.\n",
    "    Args:\n",
    "        path: string of the path of the file.\n",
    "    Returns:\n",
    "        words: dict of the words and each with a given weight.\n",
    "        max_len: maximum length of words in the dict.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(path) as f:\n",
    "        for s in f:\n",
    "            s = clean_sentence(s)\n",
    "            s = s.replace('.','')\n",
    "            X.append(s.replace(' ',''))\n",
    "            Y.append(s.split(' '))\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dictionary\n",
    "unigrams,max_length = read_unigrams(\"../data/unigrams.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'my', 'world']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    s = \"welcometomyworld\" #replace this with your sentence or if you want to use it as input just uncommenct next line\n",
    "    #s = input(\"Enter your sentence: \")\n",
    "    print(find_spaces(s,unigrams, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = read_testset(\"../data/test_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sentences: 7840\n",
      "No. of total words: 95648\n",
      "No. of correct words: 91098\n",
      "No. of completely correct sentences: 5656\n",
      "score:  95.24 %\n",
      "Time cost in seconds 9.322677850723267\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"score: \",score(X,Y,unigrams,max_length),\"%\")\n",
    "end = time.time()\n",
    "print(\"Time cost in seconds\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Hedranklifebeforespittingitout\n",
      "Actual output as string:  He drank life before spitting it out\n",
      "Actual output:  ['He', 'drank', 'life', 'before', 'spitting', 'it', 'out']\n",
      "Model output:  ['He', 'drank', 'life', 'before', 'spitting', 'it', 'out']\n",
      "No. of sentences: 1\n",
      "No. of total words: 7\n",
      "No. of correct words: 7\n",
      "No. of completely correct sentences: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to pickup an exact example of the test-set \n",
    "# s from 0 to 7839\n",
    "si = 300\n",
    "print(\"Sentence: \",X[si])\n",
    "print(\"Actual output as string: \",\" \".join((Y[si])))\n",
    "print(\"Actual output: \",Y[si])\n",
    "model_output = find_spaces(X[si],unigrams,max_length)\n",
    "print(\"Model output: \", model_output)\n",
    "score([X[si]],[Y[si]],unigrams,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
